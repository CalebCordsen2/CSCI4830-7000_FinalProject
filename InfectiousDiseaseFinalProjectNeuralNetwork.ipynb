{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDF = pd.read_csv('all_features_v1_0.csv')\n",
    "dataDF = dataDF.replace([np.inf,-np.inf], np.NaN)\n",
    "for col in dataDF.columns[dataDF.isna().any()].tolist():\n",
    "    dataDF[col] = dataDF.groupby(['date_start_period','STATEFP'])[col].transform(lambda x: x.fillna(x.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataDF.drop(columns=[\"NAME\",\"State_Name\",\"STATEFP\",\"COUNTYFP\",\"GEOID\",\"Unnamed: 0\",\"target_date_4wk\",\n",
    "                         \"target_date_3wk\",\"target_date_2wk\",\"date_end_period\",\"date_start_period\",\"date_end_lag\",\"date_start_lag\"])\n",
    "y = dataDF[['LOG_DELTA_INC_RATE_T','LOG_DELTA_INC_RATE_T_14','LOG_DELTA_INC_RATE_T_21','LOG_DELTA_INC_RATE_T_28']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=24)\n",
    "X_train_wk1Pred = X_train.drop(columns=['LOG_DELTA_INC_RATE_T','LOG_DELTA_INC_RATE_T_14','LOG_DELTA_INC_RATE_T_21','LOG_DELTA_INC_RATE_T_28'])\n",
    "X_test_wk1Pred = X_test.drop(columns=['LOG_DELTA_INC_RATE_T','LOG_DELTA_INC_RATE_T_14','LOG_DELTA_INC_RATE_T_21','LOG_DELTA_INC_RATE_T_28'])\n",
    "X_train_wk2Pred = X_train.drop(columns=['LOG_DELTA_INC_RATE_T_14','LOG_DELTA_INC_RATE_T_21','LOG_DELTA_INC_RATE_T_28'])\n",
    "X_test_wk2Pred = X_test.drop(columns=['LOG_DELTA_INC_RATE_T_14','LOG_DELTA_INC_RATE_T_21','LOG_DELTA_INC_RATE_T_28'])\n",
    "X_train_wk3Pred = X_train.drop(columns=['LOG_DELTA_INC_RATE_T_21','LOG_DELTA_INC_RATE_T_28'])\n",
    "X_test_wk3Pred = X_test.drop(columns=['LOG_DELTA_INC_RATE_T_21','LOG_DELTA_INC_RATE_T_28'])\n",
    "X_train_wk4Pred = X_train.drop(columns=['LOG_DELTA_INC_RATE_T_28'])\n",
    "X_test_wk4Pred = X_test.drop(columns=['LOG_DELTA_INC_RATE_T_28'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_wk1Pred = y_train.drop(columns=['LOG_DELTA_INC_RATE_T_14','LOG_DELTA_INC_RATE_T_21','LOG_DELTA_INC_RATE_T_28'])\n",
    "y_test_wk1Pred = y_test.drop(columns=['LOG_DELTA_INC_RATE_T_14','LOG_DELTA_INC_RATE_T_21','LOG_DELTA_INC_RATE_T_28'])\n",
    "\n",
    "y_train_wk2Pred = y_train.drop(columns=['LOG_DELTA_INC_RATE_T','LOG_DELTA_INC_RATE_T_21','LOG_DELTA_INC_RATE_T_28'])\n",
    "y_test_wk2Pred = y_test.drop(columns=['LOG_DELTA_INC_RATE_T','LOG_DELTA_INC_RATE_T_21','LOG_DELTA_INC_RATE_T_28'])\n",
    "\n",
    "y_train_wk3Pred = y_train.drop(columns=['LOG_DELTA_INC_RATE_T_14','LOG_DELTA_INC_RATE_T','LOG_DELTA_INC_RATE_T_28'])\n",
    "y_test_wk3Pred = y_test.drop(columns=['LOG_DELTA_INC_RATE_T_14','LOG_DELTA_INC_RATE_T','LOG_DELTA_INC_RATE_T_28'])\n",
    "\n",
    "y_train_wk4Pred = y_train.drop(columns=['LOG_DELTA_INC_RATE_T_14','LOG_DELTA_INC_RATE_T_21','LOG_DELTA_INC_RATE_T'])\n",
    "y_test_wk4Pred = y_test.drop(columns=['LOG_DELTA_INC_RATE_T_14','LOG_DELTA_INC_RATE_T_21','LOG_DELTA_INC_RATE_T'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
    "        super(RNNModel, self).__init__() # these are setting up the hyperparams for the RNN model\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.RNN(input_dim, hidden_dim, num_layers, batch_first=True, nonlinearity='tanh') # using tanh function for activation\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_()\n",
    "        out, hn = self.rnn(x, h0.detach())\n",
    "        out = self.fc(out[:, -1, :])  # Take last output\n",
    "        return out\n",
    "\n",
    "def train_and_predict(X_train, y_train, X_test, y_test):\n",
    "    scaler_x = MinMaxScaler()\n",
    "    scaler_y = MinMaxScaler()\n",
    "    X_train_scaled = scaler_x.fit_transform(X_train)\n",
    "    X_test_scaled = scaler_x.transform(X_test)\n",
    "    y_train_scaled = scaler_y.fit_transform(y_train)\n",
    "    y_test_scaled = scaler_y.transform(y_test)\n",
    "\n",
    "    model = RNNModel(input_dim=1, hidden_dim=512, num_layers=1, output_dim=1)\n",
    "    criterion = torch.nn.MSELoss(reduction='mean')\n",
    "    optimiser = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "    num_epochs = 100\n",
    "    X_train_tensors = torch.Tensor(X_train_scaled).unsqueeze(2)  \n",
    "    y_train_tensors = torch.Tensor(y_train_scaled)             \n",
    "\n",
    "    X_test_tensors = torch.Tensor(X_test_scaled).unsqueeze(2)   \n",
    "    y_test_tensors = torch.Tensor(y_test_scaled)                \n",
    "    y_train_tensors = y_train_tensors.view(-1, 1)  \n",
    "    y_test_tensors = y_test_tensors.view(-1, 1)   \n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        optimiser.zero_grad()\n",
    "        outputs = model(X_train_tensors)\n",
    "        loss = criterion(outputs, y_train_tensors)  \n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}')\n",
    "\n",
    "    model.eval()\n",
    "    y_test_pred = model(X_test_tensors)\n",
    "    mse = mean_squared_error(y_test_tensors.numpy(), y_test_pred.detach().numpy())\n",
    "    mae = mean_absolute_error(y_test_tensors.numpy(), y_test_pred.detach().numpy())\n",
    "    print(f'RMSE: {np.root(mse)}')\n",
    "    print(f'MAE: {mae}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes a few hours to run :)\n",
    "# train_and_predict(X_train_wk1Pred, y_train_wk1Pred, X_test_wk1Pred, y_test_wk1Pred)\n",
    "# train_and_predict(X_train_wk2Pred, y_train_wk2Pred, X_test_wk2Pred, y_test_wk2Pred)\n",
    "# train_and_predict(X_train_wk3Pred, y_train_wk3Pred, X_test_wk3Pred, y_test_wk3Pred)\n",
    "# train_and_predict(X_train_wk4Pred, y_train_wk4Pred, X_test_wk4Pred, y_test_wk4Pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
